# Conclusion 
Among all the models evaluated, LightGBM stands out as the best-performing model for predicting customer defaults. 
It achieves a good balance between recall (64%) and accuracy (71%), making it effective in identifying most default cases while maintaining a reasonable overall classification performance. 
Additionally, LightGBM outperforms other models in handling the trade-off between false positives and false negatives, which is critical in imbalanced datasets like this one.

While its precision (17%) is relatively low, this can be addressed with techniques like oversampling, undersampling, or cost-sensitive learning to further improve the modelâ€™s ability to correctly classify positive cases. Overall, LightGBM is a robust and reliable model compared to others in this analysis, demonstrating its suitability for this predictive task with further refinement and optimization.

# Future Work 
In this project, we have successfully implemented and evaluated our model to address the current problem. However, there are several areas where the model can be further enhanced to achieve better accuracy and efficiency:
* **1. Incorporating Advanced Deep Learning Techniques**

Incorporating transfer learning techniques or pre-trained models can also significantly boost performance, particularly in scenarios where labeled data is limited. Additionally, techniques like ensemble learning, combining predictions from multiple models, can help in reducing errors and improving overall robustness.
* **2. Optimizing Hyperparameter Tuning**

With access to improved computational resources, the process of hyperparameter tuning can be streamlined. Utilizing tools like grid search, random search, or advanced methods such as Bayesian optimization and automated machine learning (AutoML) can lead to identifying optimal hyperparameter settings more effectively. This would enable quicker iterations and better model performance.
* **3. Enhancing Model Training with Better Computational Resources**

The availability of higher computational resources, such as GPUs or TPUs, would allow for larger batch sizes and more complex architectures to be trained efficiently. This can also facilitate training models on larger datasets or running multiple experiments simultaneously, leading to faster convergence and improved accuracy.
* **4. Addressing Overfitting and Generalization**

Techniques like data augmentation, dropout, and batch normalization can be further explored to enhance the model's generalization ability. Additionally, fine-tuning the model with regularization techniques and cross-validation strategies can ensure better performance on unseen data.
* **5. Future Applications and Scalability**

Expanding the scope of the model by incorporating multi-task learning or domain adaptation could make it more versatile. Exploring real-time deployment scenarios and scaling the solution to handle larger datasets or diverse user requirements will be a key focus for future work.


Expanding the scope of the model by incorporating multi-task learning or domain adaptation could make it more versatile. Exploring real-time deployment scenarios and scaling the solution to handle larger datasets or diverse user requirements will be a key focus for future work.
